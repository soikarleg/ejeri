# Documentation robots.txt EJERI Jardins

## üìÑ Mise √† jour du robots.txt complet√©e

### ‚úÖ R√©sultats de la mise √† jour

- **96 pages de villes** ajout√©es au robots.txt
- **132 lignes totales** dans le fichier
- **Toutes les pages** du dossier `/services/villes/` sont autoris√©es pour l'indexation

### üìä Structure du fichier robots.txt

```
User-agent: *
Allow: /

# Sitemap
Sitemap: https://ejeri.fr/sitemap.xml

# Dossiers √† ne pas indexer
Disallow: /admin/
Disallow: /api/
Disallow: /app/
[...]

# Ressources autoris√©es
Allow: /shared/assets/
Allow: /sections/
Allow: /public/
Allow: /services/villes.php

# 96 pages de villes autoris√©es
Allow: /services/villes/aigrefeuillesurmaine.php
Allow: /services/villes/ambaresetlagrave.php
[...]
Allow: /services/villes/yvoylemarron.php

# Fichiers syst√®me
Allow: /favicon.png
Allow: /robots.txt
Allow: /sitemap.xml
```

## üéØ Impact SEO

### Avantages pour le r√©f√©rencement
1. **Indexation optimis√©e** : Toutes les pages de villes seront crawl√©es
2. **G√©olocalisation** : Am√©liore le SEO local pour chaque ville
3. **Long tail** : Capture les recherches sp√©cifiques par ville
4. **Autorit√©** : Renforce l'expertise g√©ographique du site

### Pages index√©es par r√©gion
- **Gironde (33)** : ~20 pages (Mios, Bordeaux, Talence, etc.)
- **Loiret (45)** : ~15 pages (Beaugency, Olivet, etc.)
- **Loir-et-Cher/Sologne (41)** : ~25 pages (Blois, Bracieux, etc.)
- **Vend√©e/Loire-Atlantique (85/44)** : ~35 pages (Clisson, Tiffauges, etc.)

## ü§ñ Scripts d'automatisation

### 1. update_robots.sh
Script pour mettre √† jour automatiquement le robots.txt :
```bash
./update_robots.sh
```

### 2. deploy_seo_pages.sh
Script combin√© (g√©n√©ration + robots.txt) :
```bash
./deploy_seo_pages.sh
```

### 3. Workflow recommand√©
```bash
# 1. G√©n√©rer/reg√©n√©rer toutes les pages
./generate_ville_pages.sh

# 2. Mettre √† jour le robots.txt
./update_robots.sh

# OU utiliser le script combin√©
./deploy_seo_pages.sh
```

## üîß Maintenance

### Ajout de nouvelles villes
1. Ajouter les donn√©es dans les fichiers JSON
2. Ou modifier le script `generate_ville_pages.sh`
3. Ex√©cuter `./deploy_seo_pages.sh`
4. Le robots.txt sera automatiquement mis √† jour

### V√©rification de la coh√©rence
```bash
# Comparer nombre de fichiers vs robots.txt
find services/villes/ -name "*.php" | wc -l
grep "Allow: /services/villes/" robots.txt | wc -l
```

## üìà M√©triques de suivi

### KPIs √† surveiller
- **Pages index√©es** : Search Console Google
- **Trafic local** : Analytics par ville
- **Positionnement** : "jardinier + ville" dans SERPs
- **CTR** : Taux de clic depuis les recherches locales

### Outils recommand√©s
- Google Search Console
- Google Analytics 4
- SEMrush/Ahrefs pour le suivi des positions
- Google My Business pour la coh√©rence locale

## üéØ Prochaines √©tapes

1. **Sitemap.xml** : Ajouter toutes les nouvelles pages
2. **Schema.org** : V√©rifier la coh√©rence des donn√©es structur√©es
3. **Liens internes** : Optimiser le maillage entre les pages
4. **Contenu** : Enrichir certaines pages avec plus de contenu local

## ‚úÖ Validation

Le robots.txt est maintenant optimis√© pour :
- ‚úÖ Autoriser l'indexation de toutes les pages de villes
- ‚úÖ Bloquer les dossiers sensibles (admin, api, etc.)
- ‚úÖ Permettre l'acc√®s aux ressources importantes
- ‚úÖ Faciliter le crawl des moteurs de recherche

**Total : 96 pages de villes pr√™tes pour l'indexation !**
